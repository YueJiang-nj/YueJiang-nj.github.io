<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>EyeFormer: Predicting Personalized Scanpaths with Transformer-Guided Reinforcement Learning</title>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

<!-- Meta tags for Zotero grab citation -->
<meta name="citation_title" content="EyeFormer: Predicting Personalized Scanpaths with Transformer-Guided Reinforcement Learning">
<meta name="citation_author" content="Jiang, Yue">
<meta name="citation_author" content="Guo, Zixin">
<meta name="citation_author" content="Tavakoli, Hamed R.">
<meta name="citation_author" content="Leiva, Luis A.">
<meta name="citation_author" content="Oulasvirta, Antti">
<meta name="citation_publication_date" content="2024">
<meta name="citation_conference_title" content="Proceedings of ACM Symposium on User Interface Software and Technology (UIST2024)">
<!-- <meta name="citation_volume" content="271"> -->
<!-- <meta name="citation_issue" content="20"> -->
<!-- <meta name="citation_firstpage" content="11761"> -->
<!-- <meta name="citation_lastpage" content="11766"> -->
<meta name="citation_pdf_url" content="https://yuejiang-nj.github.io/Publications/2023CHI_UEyes/paper.pdf">

<!-- Meta tags for search engines to crawl -->
<meta name="robots" content="index,follow">
<meta name="description" content="From a visual-perception perspective, modern graphical user interfaces (GUIs) comprise a complex graphics-rich two-dimensional visuospatial arrangement of text, images, and interactive objects such as buttons and menus.
While existing models can accurately predict regions and objects that are likely to attract attention ``on average'', 
no scanpath model has been capable of predicting scanpaths for an individual.
To close this gap, we introduce EyeFormer, which utilizes a Transformer architecture as a policy network to guide a deep reinforcement learning algorithm that predicts gaze locations.
Our model offers the unique capability of producing personalized predictions when given a few user scanpath samples.
It can predict full scanpath information, including fixation positions and durations, 
across individuals and various stimulus types.
Additionally, we demonstrate applications in GUI layout optimization driven by our model. 
">
<meta name="keywords" content="eye tracking, user interfaces, scanpath prediction, visual saliency">
<link rel="author" href="https://yuejiang-nj.github.io/"/>

<!-- Fonts and stuff -->
<!--<link href='http://fonts.googleapis.com/css?family=Open+Sans:400italic,700italic,800italic,400,700,800' rel='stylesheet' type='text/css'>-->
<link rel="stylesheet" type="text/css" href="css/project.css" media="screen" />
<link rel="stylesheet" type="text/css" media="screen" href="css/iconize.css" />
<!--<script src="js/google-code-prettify/prettify.js"></script>--> 


</head>

<body>
  <div id="content">
    <div id="content-inner">
      <div class="section logos">
	<a href="https://www.aalto.fi/en" target="_blank"><img width="15%" height="15%" src="aalto.png"></a>
	<a href="https://wwwen.uni.lu/" target="_blank"><img width="15%" height="15%" src="luxembourg.png"></a>
	<a href="https://www.nokia.com/fi_fi/" target="_blank"><img width="15%" height="15%" top="50px" src="nokia.png"></a>
<!-- 	<a href="https://www.cs.ubc.ca/" target="_blank"><img width="14%" height="14%" src="images/ubc.png"></a> -->
      </div>

      <div class="section head">
          <h1>UEyes: Understanding Visual Saliency across User Interface Types</h1>

	<div class="authors">
	  <a href="https://yuejiang-nj.github.io/" target="_blank">Yue Jiang</a><sup>1</sup>&#160;&#160;
	  <a href="https://luis.leiva.name/web/" target="_blank">Luis A. Leiva</a><sup>2</sup>&#160;&#160;
	  <a href="http://hrtavakoli.net/hrtavakoli.net/HOME.html" target="_blank">Hamed R. Tavakoli</a><sup>3</sup>
	  <a href="" target="_blank">Paul R. B. Houssel</a><sup>2</sup>&#160;&#160;
	  <a href="" target="_blank">Julia Kylmälä</a><sup>1</sup>&#160;&#160;
	  <a href="https://users.aalto.fi/~oulasvir/" target="_blank">Antti Oulasvirta</a><sup>1</sup>&#160;&#160;
	  <!-- &#160;&#160; -->
	 <!--  <a href="http://people.mpi-inf.mpg.de/~elgharib/" target="_blank">Mohamed Elgharib</a><sup>1,2</sup>&#160;&#160;<br />
	  <a href="https://people.epfl.ch/pascal.fua" target="_blank">Pascal Fua</a><sup>3</sup>&#160;&#160;
	  <a href="http://people.mpi-inf.mpg.de/~hpseidel/" target="_blank">Hans-Peter Seidel</a><sup>1,2</sup>&#160;&#160;
	  <a href="https://people.epfl.ch/helge.rhodin" target="_blank">Helge Rhodin</a><sup>3,4</sup>&#160;&#160;
	  <a href="http://virtualhumans.mpi-inf.mpg.de/people/pons-moll.html" target="_blank">Gerard Pons-Moll</a><sup>1,2</sup>&#160;&#160;
	  <a href="http://www.mpi-inf.mpg.de/~theobalt/" target="_blank">Christian Theobalt</a><sup>1,2</sup> -->
	</div>

	<div class="affiliations">
	  <sup>1</sup><a href="https://www.aalto.fi/en" target="_blank">Aalto University, Finland</a> <a href="https://www.aalto.fi/en" target="_blank"></a>&#160;&#160;
	  <sup>2</sup><a href="https://wwwen.uni.lu/" target="_blank">University of Luxembourg, Luxembourg</a> <a href="https://wwwen.uni.lu/" target="_blank"></a>&#160;&#160;
	  <sup>3</sup><a href="https://www.nokia.com/fi_fi/" target="_blank">Nokia Technologies, Finland</a> <a href="https://www.nokia.com/fi_fi/" target="_blank"></a>&#160;&#160;
	  <!-- <sup>2</sup><a href="https://saarland-informatics-campus.de/en/" target="_blank">Saarland Informatics Campus</a>&#160;&#160;
	  <sup>3</sup><a href="https://www.epfl.ch/en/" target="_blank">EPFL</a>&#160;&#160;
	  <sup>4</sup><a href="https://www.cs.ubc.ca/" target="_blank">University of British Columbia</a>&#160;&#160; -->
	  <br />
	</div>

	<div class="venue">Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems (<a href="https://chi2023.acm.org//" target="_blank">CHI 2023</a>), Hamburg, Germany.</div>
      </div>
	  
	  <!--	<div class="section abstract">
	  <center>
	    <h2>
	      <b>Update: The dataset is now available for download</b>
	      </h2>
	    </center>
	  <center>
	    <h2>
	      <b>Update: Live demo of VNect at CVPR17! (<a href="../VNectDemo/" target="_blank">More details</a>)</b>
	      </h2>
              </center>
          </br>
	  <center>
	    <h2>
                <b>Update: <a href="../VNectDemo/" target="_blank">VNect Demo</a> Model and Code Available For <br> Download as a <a href="#library"> C++ Library</a>! </b>
	      </h2>
	    </center>
          </div> -->


    <!--   <div class="section teaser">
          <iframe width="640" height="360" src="//www.youtube-nocookie.com/embed/W1ZNFfftx2E?showinfo=0&controls=1&modestbranding=0&autohide=1&theme=light" frameborder="0" webkitallowfullscreen mozallowfullscreen allowfullscreen></iframe> 
        <video width="70%" class="tab" controls>Your browser does not support the &lt;video&gt; tag.
              <source src="https://www.youtube.com/watch?v=l3h9JZHAOqI"/>
        </video>
 -->


	<!-- <p style="font-size:11px; text-align:center">
	  Download Video: <a href="content/XNect_SIGGRAPH2020.mp4" target="_blank">HD</a> (MP4, 1080p, 152 MB)
	</p> -->
      <!-- </div> -->

          <div class="section teaser">
	  	<iframe src="//www.youtube-nocookie.com/embed/PjWSbFbll70?showinfo=0&amp;controls=1&amp;modestbranding=0&amp;autohide=1&amp;theme=light" webkitallowfullscreen="" mozallowfullscreen="" allowfullscreen="" width="640" height="360" frameborder="0"></iframe>
	<p style="font-size:11px; text-align:center">
	  <a href="video.mp4" target="_blank">Download Video</a>(MP4, 57.7 MB)
	</p>
      </div>



      

      <div class="section abstract">
	<h2>Abstract</h2>
	<p>
		From a visual-perception perspective, modern graphical user interfaces (GUIs) comprise a complex graphics-rich two-dimensional visuospatial arrangement of text, images, and interactive objects such as buttons and menus.
		While existing models can accurately predict regions and objects that are likely to attract attention ``on average'', 
		no scanpath model has been capable of predicting scanpaths for an individual.
		To close this gap, we introduce EyeFormer, which utilizes a Transformer architecture as a policy network to guide a deep reinforcement learning algorithm that predicts gaze locations.
		Our model offers the unique capability of producing personalized predictions when given a few user scanpath samples.
		It can predict full scanpath information, including fixation positions and durations, 
		across individuals and various stimulus types.
		Additionally, we demonstrate applications in GUI layout optimization driven by our model. 
		
		</p>
      </div>



      <div class="section downloads">
	<h2>Resources and Downloads</h2>
	<center>
	  <ul>
            <li class="grid">
	      <div class = "griditem">
		<a href="paper.pdf" target="_blank" class="imageLink"><img src = "icns/pdf.png" width="10" height="10"></a><br />
		  Paper<br/><a href="paper.pdf" target="_blank">PDF</a>
		</div>
	      </li>
            <li class="grid">
	      <div class = "griditem">
		<a href="supp.pdf" target="_blank" class="imageLink"><img src = "icns/pdf.png"></a><br />
		  Supp<br/><a href="supp.pdf" target="_blank">PDF</a>
		</div>
	      </li>

	      <li class="grid"> 
	       <div class = "griditem"> 
		<a href="video.mp4" target="_blank" class="imageLink"><img src = "icns/mp4.png" ></a><br />
	     	Video<br /> <a href="video.mp4" target="_blank">MP4</a>
	       </div>
	     </li>
	    
	    <li class="grid"> 
	       <div class = "griditem"> 
		<a href="https://www.youtube.com/watch?v=BE_zVxKDSms" target="_blank" class="imageLink"><img src = "icns/mp4.png"></a><br />
	     	Video Preview<br /> <a href="video_preview.mp4" target="_blank">MP4</a>
	       </div>
	     </li>

	      </ul>

	      <br>


	      <ul>


	      <li class="grid">
	      <div class = "griditem">
		<a href="https://github.com/YueJiang-nj/EyeFormer-UIST2024" target="_blank" class="imageLink"><img src = "icns/github.png"></a><br />
		  Code<br/><a href="https://github.com/YueJiang-nj/EyeFormer-UIST2024" target="_blank">GitHub</a>
		</div>
	      </li>

	    
	   <!--  <li class="grid"> 
	       <div class = "griditem"> 
		<a href="https://docs.google.com/spreadsheets/d/12dmh9_Tx7bj1UXgM5v1KIXdID7bhqFKI73XTWEcqzmY/edit?usp=sharing" target="_blank" class="imageLink"><img src = "images/qual.jpg"></a><br />
	     	CC BY Sequences<br /> <a href="https://docs.google.com/spreadsheets/d/12dmh9_Tx7bj1UXgM5v1KIXdID7bhqFKI73XTWEcqzmY/edit?usp=sharing" target="_blank">Links</a>
	       </div>
	     </li> -->
             
	    </ul>
	    </center>
	    </div>
<br />


 <div class="section list">
	<h2>Citation</h2>
	<p><a href="reference.bib" target="_blank">BibTeX, 1 KB</a></p>
	<div class="section bibtex">
	      <pre>
			@inproceedings{jiang2024eyeformer,
				author = {Jiang, Yue and Guo, Zixin and Rezazadegan Tavakoli, Hamed and Leiva, Luis A. and Oulasvirta, Antti},
				title = {EyeFormer: Predicting Personalized Scanpaths with Transformer-Guided Reinforcement Learning},
				year = {2024},
				isbn = {9798400706288},
				publisher = {Association for Computing Machinery},
				address = {New York, NY, USA},
				url = {https://doi.org/10.1145/3654777.3676436},
				doi = {10.1145/3654777.3676436},
				abstract = {From a visual-perception perspective, modern graphical user interfaces (GUIs) comprise a complex graphics-rich two-dimensional visuospatial arrangement of text, images, and interactive objects such as buttons and menus. While existing models can accurately predict regions and objects that are likely to attract attention “on average”, no scanpath model has been capable of predicting scanpaths for an individual. To close this gap, we introduce EyeFormer, which utilizes a Transformer architecture as a policy network to guide a deep reinforcement learning algorithm that predicts gaze locations. Our model offers the unique capability of producing personalized predictions when given a few user scanpath samples. It can predict full scanpath information, including fixation positions and durations, across individuals and various stimulus types. Additionally, we demonstrate applications in GUI layout optimization driven by our model.},
				booktitle = {Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology},
				articleno = {47},
				numpages = {15},
				location = {Pittsburgh, PA, USA},
				series = {UIST '24}
				}
</pre>
	  </div>
 </div>
   
      <div class="section contact">
	<h2>Contact</h2>
        <a name="contact"></a>
        For questions and clarifications, please get in touch with:<br />
        Yue Jiang <a href="mailto:yuenj.jiang@gmail.com">yuenj.jiang@gmail.com</a> <br />
      </div>

      <div class="section">
	<hr class="smooth">

	  <!-- <a href="http://www.zotero.org" target="_blank">Zotero</a> translator friendly. |  -->
          <!--Page last updated--> 
              <!--#config timefmt="%d-%b-%Y" --><!--#echo var="LAST_MODIFIED" -->
              <!-- <a href="https://imprint.mpi-klsb.mpg.de/inf/xnect">Imprint</a> | -->
              <!-- <a href="https://data-protection.mpi-klsb.mpg.de/inf/xnect">Data Protection</a>  -->
      </div>

    </div>
  </div>
</div>

</body>
</html>
